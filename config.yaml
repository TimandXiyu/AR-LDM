# device
mode: train_unseen
gpu_ids: [2,3]  # gpu ids
batch_size: 1 # each item denotes one story
num_workers: 8 # number of workers
num_cpu_cores: -1  # number of cpu cores
grad_accumulation_steps: 1 # gradient accumulation steps
seed: 0  # random seed
ckpt_dir: ./ckpts # checkpoint directory
run_name: flintstones_us10_more_ref # name for this run
cur_char: # cache current character
history_char: # cache history character, don't fill this field
char_tokens: # cache unseen character tokens
inject_lora: False # whether to inject lora
distillation: True # whether to use distillation
distillation_weight: 1.0 # distillation weight

# task
dataset: flintstones_unseen # pororo flintstones vistsis vistdii | flintstones_unseen
task: visualization  # continuation visualization

# train
init_lr: 1e-5
warmup_epochs: 5  # warmup epochs
max_epochs: 100 # max epochs
train_model_file:  # model file for resume, none for train from scratch
freeze_clip: False  # whether to freeze clip
freeze_blip: False  # whether to freeze blip
freeze_resnet: False  # whether to freeze resnet
save_freq: 100  # save frequency

# sample
test_model_file: "/home/xiyu/projects/AR-LDM/ckpts/flintstones_pretrain/epoch=49-step=225100.ckpt" #model file for test
calculate_fid: False  # whether to calculate FID scores
calculate_text_clipscore: False # whether to calculate text clipscore
calculate_visual_clipscore: False # whether to calculate visual clipscore, need to have reference images
scheduler: ddim  # ddim pndm
guidance_scale: 6  # guidance scale
num_inference_steps: 100  # number of inference steps
sample_output_dir: ./ckpts/output_images # output directory
custom_prompts: ./datasets/fs_special_token.txt # custom prompts file, only used for custom_sample
stop_sample_early: # only sample for a few stories
resolution: 256 # resolution for sampling, keep the same as the training resolution

# model config
unet_model:
  tuning:
  low_cpu_mem_usage: False

flintstones_unseen:
  target_chars: ["slaghoople", "texarock_2", "gazoo", "theft", "yellow_tiger", "helmet_police", "piano_man", "red_hair_lady", "rockzilla", "texarock_1"]
  data_dir: ./data/flintstones_data
  target_dir: ./data/flintstones_data/target_chars
  hdf5_file: ./data/flintstones_rare-char_rmeoved.h5
  max_length: 91
  new_tokens: [ "fred", "barney", "wilma", "betty", "dino", "slate" ]
  clip_embedding_tokens: 49412
  blip_embedding_tokens: 30525
  use_handpick: True # using manual followings

pororo_unseen:
  target_chars: ['tutu', 'alien', 'shark', 'whale'] # ['popo', 'pipi', 'whale', 'shark', 'harry', 'tutu', 'alien']
  data_dir: ./data/pororo_png
  target_dir: ./data/pororo_png/target_chars
  hdf5_file: ./data/pororo_char_removed.h5
  max_length: 85
  new_tokens: [ "pororo", "loopy", "eddy", "poby", "tongtong", "crong", "rody", "petty"]
  clip_embedding_tokens: 49416
  blip_embedding_tokens: 30530

flintstones:
  hdf5_file: ./data/flintstones_rare-char_rmeoved.h5
  max_length: 91
  new_tokens: [ "fred", "barney", "wilma", "betty", "dino", "slate"]
  adapt_tokens:
  clip_embedding_tokens: 49412
  blip_embedding_tokens: 30525

pororo:
  hdf5_file: ./data/pororo_char_removed.h5
  max_length: 85
  new_tokens: [ "pororo", "loopy", "eddy", "poby", "tongtong", "crong", "rody", "petty"]
  adapt_tokens:
  clip_embedding_tokens: 49416
  blip_embedding_tokens: 30530



vistsis:
  hdf5_file: /path/to/vist.h5
  max_length: 100
  clip_embedding_tokens: 49408
  blip_embedding_tokens: 30524

vistdii:
  hdf5_file: /path/to/vist.h5
  max_length: 65
  clip_embedding_tokens: 49408
  blip_embedding_tokens: 30524

hydra:
  run:
    dir: .
  output_subdir: null
hydra/job_logging: disabled
hydra/hydra_logging: disabled